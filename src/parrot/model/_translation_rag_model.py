"""
Translation RAG Model Module

"""

import numpy as np
from typing import List, Optional, Tuple
from sentence_transformers import SentenceTransformer

Faiss = None


def lazyFaiss():
    global Faiss
    if Faiss is None:
        import faiss

        Faiss = faiss
        print("âœ“ FAISS loaded successfully.")
    return Faiss


class TranslationRagModel:
    def __init__(self):
        # RAG ê´€ë ¨ ì´ˆê¸°í™”
        self.embedding_model = SentenceTransformer(
            "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
        )
        print("âœ“ Embedding Model loaded successfully.")
        self.terminology_db = {}
        self.faiss_index = None
        self.term_embeddings = []
        self.term_pairs = []

    def load_terminology_db(self) -> None:
        # ìš©ì–´ ë§¤í•‘ ì •ì˜
        self.terminology_db = {
            "ko2ja": [
                ("í¬ì¹´", "ãƒ•ã‚©ãƒˆã‚«ãƒ¼ãƒ‰"),
                ("êµ¿ì¦ˆ", "ã‚°ãƒƒã‚º"),
                ("ë•ì§ˆ", "æ¨ã—æ´»"),
                ("ë¶€ìº", "å‰¯å¢"),
            ],
            "ja2ko": [
                ("ãƒ•ã‚©ãƒˆã‚«ãƒ¼ãƒ‰", "í¬ì¹´"),
                ("ã‚°ãƒƒã‚º", "êµ¿ì¦ˆ"),
                ("æ¨ã—æ´»", "ë•ì§ˆ"),
                ("å‰¯å¢", "ë¶€ìº"),
            ],
            "ko2ko": [
                ("í¬ì¹´", "í¬í† ì¹´ë“œ"),
                ("êµ¿ì¦ˆ", "êµ¿ì¦ˆ"),
                ("ë•ì§ˆ", "ì˜¤íƒ€ì¿ "),
                ("ë¶€ìº", "ë¶€ ìºë¦­í„°"),
            ],
        }
        self.build_index()

    def get_domain_from_lang(
        self,
        source_lang: str,
        target_lang: str,
        use_replacement: bool = False,
    ) -> str:
        if source_lang == "korean" and target_lang == "japanese":
            return "ko2ja" if not use_replacement else "ko2ko"
        elif source_lang == "japanese" and target_lang == "korean":
            return "ja2ko" if not use_replacement else "ja2ja"
        else:
            return "ko2ja"

    def build_index(self):
        """FAISS ì¸ë±ìŠ¤ êµ¬ì¶•"""
        all_terms = []
        all_pairs = []

        for domain, term_pairs in self.terminology_db.items():
            for source_term, target_term in term_pairs:
                all_terms.append(source_term)
                all_pairs.append((source_term, target_term, domain))

        if all_terms:
            # ì„ë² ë”© ìƒì„±
            embeddings = self.embedding_model.encode(all_terms)

            # FAISS ì¸ë±ìŠ¤ êµ¬ì¶•
            dimension = embeddings.shape[1]
            self.faiss_index = lazyFaiss().IndexFlatIP(dimension)  # ì½”ì‚¬ì¸ ìœ ì‚¬ë„

            # ì •ê·œí™” í›„ ì¸ë±ìŠ¤ì— ì¶”ê°€
            embeddings_normalized = embeddings / np.linalg.norm(
                embeddings, axis=1, keepdims=True
            )
            self.faiss_index.add(embeddings_normalized.astype("float32"))

            self.term_embeddings = embeddings_normalized
            self.term_pairs = all_pairs

        print(f"âœ“ Terminology database loaded: {len(self.term_pairs)} terms")

    def retrieve_terminology(
        self,
        text: str,
        domain: Optional[str] = None,
        k: int = 5,
        threshold: float = 0.7,
    ) -> List[Tuple[str, str, str, float]]:
        """ê´€ë ¨ ìš©ì–´ ê²€ìƒ‰ - ë„ë©”ì¸ í•„í„°ë§ í¬í•¨"""
        if not self.faiss_index:
            return []

        words = text.split()
        retrieved_terms = []

        for word in words:
            if len(word) >= 2:
                query_embedding = self.embedding_model.encode([word])
                query_normalized = query_embedding / np.linalg.norm(query_embedding)

                scores, indices = self.faiss_index.search(
                    query_normalized.astype("float32"), k
                )

                for score, idx in zip(scores[0], indices[0]):
                    if score > threshold and idx < len(self.term_pairs):
                        source_term, target_term, term_domain = self.term_pairs[idx]
                        # ë„ë©”ì¸ í•„í„°ë§ ì ìš©
                        if domain is None or term_domain == domain:
                            retrieved_terms.append(
                                (source_term, target_term, term_domain, score)
                            )

        # ì¤‘ë³µ ì œê±° (highest scoreë§Œ ìœ ì§€)
        unique_terms = {}
        for source_term, target_term, term_domain, score in retrieved_terms:
            key = (source_term, target_term, term_domain)
            if key not in unique_terms or unique_terms[key][3] < score:
                unique_terms[key] = (source_term, target_term, term_domain, score)

        # ì ìˆ˜ ìˆœìœ¼ë¡œ ì •ë ¬
        return sorted(unique_terms.values(), key=lambda x: x[3], reverse=True)

    def retrieve_replace_text_with_domain(
        self,
        text: str,
        domain: Optional[str] = None,
    ) -> str:
        # 1. ë„ë©”ì¸ë³„ ê´€ë ¨ ìš©ì–´ ê²€ìƒ‰
        retrieved_terms = self.retrieve_terminology(text, domain=domain)

        # ì›ë¬¸ì—ì„œ íŠ¹ìˆ˜ ìš©ì–´ë¥¼ ì¼ë°˜ì ì¸ ë‹¨ì–´ë¡œ êµì²´
        preprocessed_text = text
        for source_term, target_term, _, _ in retrieved_terms:
            if source_term in preprocessed_text:
                # ì„ì‹œë¡œ ì¼ë°˜ì ì¸ ë‹¨ì–´ë¡œ êµì²´ (ë²ˆì—­ì´ ì˜ ë˜ë„ë¡)
                preprocessed_text = preprocessed_text.replace(
                    source_term, f"{target_term}"
                )
        return preprocessed_text

    def retrieve_text_with_domain(
        self,
        text: str,
        domain: Optional[str] = None,
        max_terms: int = 3,  # ğŸ”§ ì¡°ì • ê°€ëŠ¥í•˜ê²Œ
    ) -> str:
        # 1. ë„ë©”ì¸ë³„ ê´€ë ¨ ìš©ì–´ ê²€ìƒ‰
        retrieved_terms = self.retrieve_terminology(text, domain=domain)

        if not retrieved_terms:
            return text  # ìš©ì–´ê°€ ì—†ìœ¼ë©´ ì›ë³¸ ë°˜í™˜

        # ë” ìì—°ìŠ¤ëŸ¬ìš´ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        terminology_context = ", ".join(
            [
                f"'{source}' â†’ '{target}'"
                for source, target, _, _ in retrieved_terms[:max_terms]
            ]
        )

        context_text = f"""{terminology_context}"""

        return context_text
